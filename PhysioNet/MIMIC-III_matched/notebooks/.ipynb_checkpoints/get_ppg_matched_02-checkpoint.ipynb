{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "93d3e857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "7b9f33ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_path_local: ../../../2_data/record_path_mimic3_matched.csv\n",
      "record_root_url: https://physionet.org/content/mimic3wdb/1.0/matched\n",
      "record_path_url: https://physionet.org/content/mimic3wdb/1.0/matched/RECORDS-waveforms\n"
     ]
    }
   ],
   "source": [
    "record_path_local = '../../../2_data/record_path_mimic3_matched.csv'\n",
    "record_root_url = 'https://physionet.org/content/mimic3wdb/1.0/matched'\n",
    "record_path_url = record_root_url + '/RECORDS-waveforms'\n",
    "\n",
    "#record_root_url = 'https://archive.physionet.org/physiobank/database/mimic3wdb'\n",
    "#record_path_url = record_root_url + '/RECORDS'\n",
    "\n",
    "print('record_path_local: {}'.format(record_path_local))\n",
    "print('record_root_url: {}'.format(record_root_url))\n",
    "print('record_path_url: {}'.format(record_path_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6620d428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_content(url, tag=None):\n",
    "    \n",
    "    '''\n",
    "    This function scrape a list of useful information from a given PhysioNet URL.\n",
    "    If the URL address points to an HTML document, the information to be extracted is define by a tag.\n",
    "    I found this address (https://hackersandslackers.com/scraping-urls-with-beautifulsoup/) usefull\n",
    "      for details on how to scrape a web page\n",
    "    '''\n",
    "    \n",
    "    headers = {\n",
    "        'Access-Control-Allow-Origin': '*',\n",
    "        'Access-Control-Allow-Methods': 'GET',\n",
    "        'Access-Control-Allow-Headers': 'Content-Type',\n",
    "        'Access-Control-Max-Age': '3600',\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'\n",
    "    }    \n",
    "    \n",
    "    req = requests.get(url, headers)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    if 'DOCTYPE' in str(soup):\n",
    "        content = getattr(soup, str(tag)).getText()\n",
    "        content = soup.pre.getText()\n",
    "    else:\n",
    "        content = soup.getText()\n",
    "    return content.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2371336a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baubau_1\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(record_path_local):\n",
    "    record_path_list = pd.read_csv(record_path_local, header=None)\n",
    "    record_path_list = list(record_path_list.iloc[:,0])\n",
    "    print('baubau_1')\n",
    "else:\n",
    "    target_url = record_path_url\n",
    "    record_path_list = get_url_content(target_url, tag)\n",
    "    pd.DataFrame(record_path_list).to_csv(record_path_local, header=None, index=None)\n",
    "    print('baubau_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69371120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
